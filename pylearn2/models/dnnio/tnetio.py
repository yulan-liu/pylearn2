"""
Copyright 2014-2016 Yulan Liu

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
MERCHANTABLITY OR NON-INFRINGEMENT.
See the Apache 2 License for the specific language governing permissions and
limitations under the License.
"""

# Last updated: 5 Aug 2014 by Yulan Liu.

import os
from numpy import *
from pylearn2.models.mlp import MLP, Linear, Softmax, Sigmoid

def read_tnet(innetfile=None):
    """
    Function: read DNN file generated by TNET (developped by Burno University).

    Note that float32 is by default used because theano does not support
    proper speedup with float 64 yet.

    Be careful with the shape of matrix. The implementation here assumes
    that matrix multiplication is done in the following way:

	W*X + b

    where W is the DNN linear weight matrix, b is the bias vector, X is
    the (hidden) input features for this layer.

    Returned DNN coefficients are in lists of numpy arrays, in float (not
    necessarily float32 or float64).
    """
    fid = open(innetfile, 'r')
    structure = []
    matrixflag = False
    biasflag = False
    weights = []
    bias = []
    vecw = []
    act = []            # TNET function for each layer
    ord = []
    for line in fid:
        line = line.strip()
        if line=='':
            continue
        data = line.split()
        if line[0]=='<':
            act.append(data[0][1:-1])
            matrixflag = False
            biasflag = False
            if data[0]=='<biasedlinearity>':
                nhid, nvis = data[1:]
#                structure.append([nvis, nhid])
		structure.append([int(nvis), int(nhid)])
            elif data[0]=='<sigmoid>':
                weights.append(array(vecw, dtype=float))
                vecw = []
        elif data[0]=='m':
            matrixflag = True
	    biasflag = False
	    ord.append(['m', 'weights'])
            continue
        elif data[0]=='v':
            biasflag = True
	    matrixflag = False
	    ord.append(['v', 'bias'])
            continue
        elif matrixflag:
            vecw.append(data)
        elif biasflag:
            bias.append(array(data, dtype=float))
    weights.append(array(vecw, dtype=float))
    fid.close()

    TNETdnn = {'weights': weights, 'bias': bias, 'structure':structure, 'act': act, 'ord': ord}

    return TNETdnn 


def write_tnet(outfile=None, TNETdnn=None):
    """
    Save DNN into a format compatible with TNET software developped by
    Burno University. DNN parameters are saved in a dictionary format
    defined by "read_tnet" function.
    """
    fid = open(outfile, 'w')
    
    N = len(TNETdnn['act'])
    i = 0
    while i<N:
	j = int(i/2)

	# Write the weight matrix
	fid.write('<' + TNETdnn['act'][i] + '> ' )
	tmp = str(TNETdnn['structure'][j][1]) + ' ' + str(TNETdnn['structure'][j][0]) + '\n'
	fid.write(tmp)
	fid.write(TNETdnn['ord'][i][0] + ' ' + tmp)
	for w1 in TNETdnn[TNETdnn['ord'][i][1]][j]:
	    flag = False
	    for w2 in w1:
		if flag:
		    fid.write(' ')
		fid.write(str(w2))
		flag = True
	    fid.write('\n')

	# Write the bias
	fid.write(TNETdnn['ord'][i+1][0] + ' ' + str(TNETdnn['structure'][j][1]) + '\n')
	flag = False
    	for w1 in TNETdnn[TNETdnn['ord'][i+1][1]][j]:
	    if flag:
		fid.write(' ')
	    fid.write(str(w1))
	    flag = True
	fid.write('\n')

	# Write the activation function
	fid.write('<' + TNETdnn['act'][i+1] + '> ' )
	fid.write(str(TNETdnn['structure'][j][1]) + ' ' + str(TNETdnn['structure'][j][1]) + '\n')

	i += 2
    fid.close()
    return 0

def dnn_p2t(innet=None):
    """
    Convert the internal DNN format used in pylearn2 scripts into the
    TNET DNN format, used for reading or writing into a TNET compatible
    way.
    """
    l = innet.layers
    nlayers = len(l)

    TNETdnn = {'ord':[], 'bias':[], 'weights':[], 'structure':[], 'act':[]}
    for i in range(0, nlayers):
	layer_name = l[i].layer_name
	W = l[i].get_weights()
	b = l[i].get_biases()
	name = layer_name.split('_')[-1]
	tmp = shape(W)
	TNETdnn['structure'].append([tmp[0], tmp[1]])
	TNETdnn['weights'].append(W)
	TNETdnn['bias'].append(b)
	if name=='sig':
	    TNETdnn['ord'].append(['m', 'weights'])
	    TNETdnn['ord'].append(['v', 'bias'])
	    TNETdnn['act'].append('biasedlinearity')
	    TNETdnn['act'].append('sigmoid')
	elif name=='sfm':
            TNETdnn['ord'].append(['m', 'weights'])
            TNETdnn['ord'].append(['v', 'bias'])
            TNETdnn['act'].append('biasedlinearity')
            TNETdnn['act'].append('softmax')
	else:
	    print 'Layer name not recognized, currently only support *_sig for sigmoid layer name and *_sfm for softmax layer name.'
	    raise NotImplementedError

    return TNETdnn

def dnn_t2p(TNETdnn=None, randomize=False):
    """
    Convert the TNET DNN format (from reading or writing TNET formated
    DNN files) into the internal format used by all pylearn2 scripts.

    If randomize=True, the weights from TNETdnn will be replaced with
    internally generated random values and the biases will be set to 0.
    However the layer structure (sigmoid or softmax, number of neurons
    in each layer, etc.) will be inheritted.
    """
    structure = TNETdnn['structure']
    weights = TNETdnn['weights']
    nlayers = shape(weights)[0]
    bias = TNETdnn['bias']
    act = TNETdnn['act']

    s = []

    for i in range(0, nlayers):
	if (act[i*2]=='biasedlinearity') and (act[i*2+1]=='sigmoid'):
	    s.append(Sigmoid(dim=structure[i][1], layer_name='h'+str(i)+'_sig', irange=0.005))
	elif (act[i*2]=='biasedlinearity') and (act[i*2+1]=='softmax'):
	    s.append(Softmax(n_classes=structure[i][1], layer_name='h'+str(i)+'_sfm', irange=0.005))
	else:
	    raise NotImplementedError()

    # Attach all the layers to an MLP "innet"
    innet = MLP(nvis=structure[0][0], layers=s)

    if randomize:
	return innet

    # Re-write all the weights and biases with the valuese from TNET innet file 
    for i in range(0, nlayers):
	W = weights[i]
	b = bias[i]
	if (not W.dtype=='float32'):
	    print 'Warning: the weights were in ', W.dtype, ', will be converted into float32. This might cause accuracy change, but theano and pylearn2 support float32 better.'
	    W = W.astype(float32)
	if (not b.dtype=='float32'):
	    print 'Warning: the biases were in ', b.dtype, ', will be converted into float32. This might cause accuracy change, but theano and pylearn2 support float32 better.'
	    b = b.astype(float32)

        s[i].set_weights(W)
        s[i].set_biases(b)

    return innet


def load_tnet_innet(innetfile=None):
    """
    Load the DNN generated in TNET format.
    """
    TNETdnn = read_tnet(innetfile)

    return dnn_t2p(TNETdnn)


# TODO:	save as hdf5 format
